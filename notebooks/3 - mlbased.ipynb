{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It is a feature exrtraction technique, which is commonly used in text mining and information retrieval. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "- Term Frequency (TF): This measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization.\n",
    "$$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}$$\n",
    "- Inverse Document Frequency (IDF): This measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "$$IDF(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents with term t in it}}\\right)$$\n",
    "- N is the total number of documents in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/AERA02_AptitudeAssessment_Dataset_NLP_cleaned_vi.csv\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n",
    "with open(\"../data/external/stopwords.txt\") as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "\n",
    "def process_text(text):\n",
    "    text = re.sub(\"(&#\\d+;)\", \"\", text)\n",
    "    text = re.sub(\"([\\/-])\", \" \", text)\n",
    "    text = re.sub(\"(<.*?>)\", \"\" ,text)\n",
    "    text = re.sub(\"(^https?:\\/\\/\\S+)\", \"\", text)\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation + \"…\"])\n",
    "    text = text.lower()\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    _WORD_SPLIT = re.compile(\"([.,!?\\\"/':;)(])\")\n",
    "    def basic_tokenizer(sentence):\n",
    "        words = []\n",
    "        for space_separated_fragment in sentence.strip().split():\n",
    "            words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "        return [w.lower() for w in words if w != '' and w != ' ' and w not in string.punctuation]\n",
    "    \n",
    "    corpus = corpus.replace(\"\\n\", \" \").split(\" \")\n",
    "    \n",
    "    return \" \".join(basic_tokenizer(\" \".join(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \" \".join(df[\"title2review\"].apply(process_text).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trải nghiệm tốt đầy đủ dịch vụ tiện nghi ăn sáng b'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(max_features=3000,sublinear_tf=True)\n",
    "tf.fit([sentences])\n",
    "X = tf.transform([sentences])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Binarizer\n\u001b[1;32m      2\u001b[0m binaray \u001b[38;5;241m=\u001b[39m Binarizer(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m binaray\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43my_score\u001b[49m)\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binaray = Binarizer(threshold=6)\n",
    "y = binaray.fit_transform(y_score)\n",
    "y = np.array(y).flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
