{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu116 torchaudio==0.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# !pip install transformers==4.35.2\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/AERA02_AptitudeAssessment_Dataset_NLP_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def process_text(text):\n",
    "    text = re.sub(\"(&#\\d+;)\", \"\", text)\n",
    "    text = re.sub(\"([\\/-])\", \" \", text)\n",
    "    text = re.sub(\"(<.*?>)\", \"\" ,text)\n",
    "    text = re.sub(\"(^https?:\\/\\/\\S+)\", \"\", text)\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation + \"…\"])\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    _WORD_SPLIT = re.compile(\"([.,!?\\\"/':;)(])\")\n",
    "    def basic_tokenizer(sentence):\n",
    "        words = []\n",
    "        for space_separated_fragment in sentence.strip().split():\n",
    "            words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "        return [w.lower() for w in words if w != '' and w != ' ' and w not in string.punctuation]\n",
    "    \n",
    "    corpus = corpus.replace(\"\\n\", \" \").split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>TRẢI NGHIỆM TỐT</td>\n",
       "      <td>Đầy đủ dịch vụ tiện nghi Ăn sáng buffee ngon H...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Tuyệt vời</td>\n",
       "      <td>Khách sạn mới, sạch sẽ, có bar và bể bơi ở tần...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>trải nghiệm tuyệt vời tại Brandi Gate</td>\n",
       "      <td>Khách sạn mới 100% tọa lạc trước sông Tô Lịch,...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>Good hotel, good room rates</td>\n",
       "      <td>During the last visit to Hanoi, in April 2019,...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>Tồi , lừa đảo</td>\n",
       "      <td>Mình đặt 2 phòng ở 3 đêm từ 30/11-3/12 . Vì có...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813623</th>\n",
       "      <td>5</td>\n",
       "      <td>Lần thứ 2 quay lại</td>\n",
       "      <td>Vừa rồi tham gia cuộc thi sắc đẹp cho doanh nh...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813648</th>\n",
       "      <td>4</td>\n",
       "      <td>Giá rẻ nhân viên thân thiện</td>\n",
       "      <td>Gia đình chúng tôi gồm bố mẹ và 1 bé 4 tuổi đã...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813651</th>\n",
       "      <td>5</td>\n",
       "      <td>Giá rẻ, đồ ăn ngon</td>\n",
       "      <td>Thấy khách sạn lâu rồi mà không dám vào ở, sợ ...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813654</th>\n",
       "      <td>5</td>\n",
       "      <td>Kỳ nghỉ tháng 10 năm 2017 tại Đà Nẵng</td>\n",
       "      <td>Khách sạn với nội thất tuyệt vời , phòng rất r...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813667</th>\n",
       "      <td>5</td>\n",
       "      <td>Công tác</td>\n",
       "      <td>Rất tuyệt vời... khi đến đây tôi cảm giác thoả...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48115 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                  title  \\\n",
       "3           5                        TRẢI NGHIỆM TỐT   \n",
       "8           5                              Tuyệt vời   \n",
       "9           5  trải nghiệm tuyệt vời tại Brandi Gate   \n",
       "16          5            Good hotel, good room rates   \n",
       "64          1                          Tồi , lừa đảo   \n",
       "...       ...                                    ...   \n",
       "813623      5                     Lần thứ 2 quay lại   \n",
       "813648      4            Giá rẻ nhân viên thân thiện   \n",
       "813651      5                     Giá rẻ, đồ ăn ngon   \n",
       "813654      5  Kỳ nghỉ tháng 10 năm 2017 tại Đà Nẵng   \n",
       "813667      5                               Công tác   \n",
       "\n",
       "                                                   review language  \n",
       "3       Đầy đủ dịch vụ tiện nghi Ăn sáng buffee ngon H...       vi  \n",
       "8       Khách sạn mới, sạch sẽ, có bar và bể bơi ở tần...       vi  \n",
       "9       Khách sạn mới 100% tọa lạc trước sông Tô Lịch,...       vi  \n",
       "16      During the last visit to Hanoi, in April 2019,...       vi  \n",
       "64      Mình đặt 2 phòng ở 3 đêm từ 30/11-3/12 . Vì có...       vi  \n",
       "...                                                   ...      ...  \n",
       "813623  Vừa rồi tham gia cuộc thi sắc đẹp cho doanh nh...       vi  \n",
       "813648  Gia đình chúng tôi gồm bố mẹ và 1 bé 4 tuổi đã...       vi  \n",
       "813651  Thấy khách sạn lâu rồi mà không dám vào ở, sợ ...       vi  \n",
       "813654  Khách sạn với nội thất tuyệt vời , phòng rất r...       vi  \n",
       "813667  Rất tuyệt vời... khi đến đây tôi cảm giác thoả...       vi  \n",
       "\n",
       "[48115 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_df = df[df[\"language\"] == \"vi\"].copy()\n",
    "vi_df[\"score\"] = vi_df[\"score\"].astype(\"int\")\n",
    "vi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "class Config():\n",
    "    seed_val = 17\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 5 \n",
    "    batch_size = 32\n",
    "    seq_length = 512\n",
    "    lr = 2e-5\n",
    "    eps = 1e-8\n",
    "    pretrained_model = 'bert-base-uncased'\n",
    "    test_size=0.15\n",
    "    random_state=42\n",
    "    add_special_tokens=True \n",
    "    return_attention_mask=True \n",
    "    pad_to_max_length=True \n",
    "    do_lower_case=False\n",
    "    return_tensors='pt'\n",
    "    cache_dir=\"/space/hotel/phit/personal/aera02-aisia/notebooks/cache\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# params will be saved after training\n",
    "params = {\"seed_val\": config.seed_val,\n",
    "    \"device\":str(config.device),\n",
    "    \"epochs\":config.epochs, \n",
    "    \"batch_size\":config.batch_size,\n",
    "    \"seq_length\":config.seq_length,\n",
    "    \"lr\":config.lr,\n",
    "    \"eps\":config.eps,\n",
    "    \"pretrained_model\": config.pretrained_model,\n",
    "    \"test_size\":config.test_size,\n",
    "    \"random_state\":config.random_state,\n",
    "    \"add_special_tokens\":config.add_special_tokens,\n",
    "    \"return_attention_mask\":config.return_attention_mask,\n",
    "    \"pad_to_max_length\":config.pad_to_max_length,\n",
    "    \"do_lower_case\":config.do_lower_case,\n",
    "    \"return_tensors\":config.return_tensors,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_, test_df = train_test_split(vi_df, \n",
    "                                      test_size=0.10, \n",
    "                                      random_state=config.random_state, \n",
    "                                      stratify=vi_df.score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed_val):\n",
    "    # set random seed and device\n",
    "    import random\n",
    "\n",
    "    device = config.device\n",
    "\n",
    "    random.seed(config.seed_val)\n",
    "    np.random.seed(config.seed_val)\n",
    "    torch.manual_seed(config.seed_val)\n",
    "    torch.cuda.manual_seed_all(config.seed_val)\n",
    "    \n",
    "set_random_seed(config.seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df_, \n",
    "                                    test_size=0.10, \n",
    "                                    random_state=42, \n",
    "                            stratify=train_df_.score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.pretrained_model, \n",
    "                                          do_lower_case=config.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376018</th>\n",
       "      <td>5</td>\n",
       "      <td>Trải nghiệm thật tuyệt vời tại Hoàn Mỹ Resort ...</td>\n",
       "      <td>Cam ơn Hoàn Mỹ resort, đã cho mình có kì nghỉ ...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249432</th>\n",
       "      <td>5</td>\n",
       "      <td>Khách sạn thân thiện</td>\n",
       "      <td>Nhân dịp nghỉ dưỡng cuối năm ở Quảng Bình đã c...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>5</td>\n",
       "      <td>Một trải nghiệm tuyệt vời tại A25 Sahul Hotel ...</td>\n",
       "      <td>Tôi đến Hà Nội công tác, nhận phòng hơi muộn. ...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354875</th>\n",
       "      <td>5</td>\n",
       "      <td>Good</td>\n",
       "      <td>Gia đình chúng tôi rất vui- hạnh phúc- tuyệt v...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397962</th>\n",
       "      <td>3</td>\n",
       "      <td>Giá tốt, chấp nhận được</td>\n",
       "      <td>Phòng sạch sẽ, giá hợp lí, quảng cáo trên inte...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591070</th>\n",
       "      <td>4</td>\n",
       "      <td>A nice destination</td>\n",
       "      <td>Tất cả đều mới, đẹp và nên thơ. Phòng rộng, th...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332938</th>\n",
       "      <td>4</td>\n",
       "      <td>Công ty</td>\n",
       "      <td>Thái độ nhân viên tốt. Chu đáo hỗ trợ khách nh...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392883</th>\n",
       "      <td>5</td>\n",
       "      <td>Tuyệt vời</td>\n",
       "      <td>Ấn tượng đầu tiên là resort cảnh quan đẹp. Sân...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113208</th>\n",
       "      <td>5</td>\n",
       "      <td>Bữa trưa tại tầng 62</td>\n",
       "      <td>Tôi đến dùng bữa trưa ở khách sạn. khách sạn c...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375793</th>\n",
       "      <td>5</td>\n",
       "      <td>Tuyệt tác giữa núi Chúa</td>\n",
       "      <td>Các villa hòa vào thiên nhiên của núi Chúa, tư...</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                              title  \\\n",
       "376018      5  Trải nghiệm thật tuyệt vời tại Hoàn Mỹ Resort ...   \n",
       "249432      5                              Khách sạn thân thiện    \n",
       "848         5  Một trải nghiệm tuyệt vời tại A25 Sahul Hotel ...   \n",
       "354875      5                                               Good   \n",
       "397962      3                            Giá tốt, chấp nhận được   \n",
       "...       ...                                                ...   \n",
       "591070      4                                 A nice destination   \n",
       "332938      4                                            Công ty   \n",
       "392883      5                                          Tuyệt vời   \n",
       "113208      5                               Bữa trưa tại tầng 62   \n",
       "375793      5                            Tuyệt tác giữa núi Chúa   \n",
       "\n",
       "                                                   review language  \n",
       "376018  Cam ơn Hoàn Mỹ resort, đã cho mình có kì nghỉ ...       vi  \n",
       "249432  Nhân dịp nghỉ dưỡng cuối năm ở Quảng Bình đã c...       vi  \n",
       "848     Tôi đến Hà Nội công tác, nhận phòng hơi muộn. ...       vi  \n",
       "354875  Gia đình chúng tôi rất vui- hạnh phúc- tuyệt v...       vi  \n",
       "397962  Phòng sạch sẽ, giá hợp lí, quảng cáo trên inte...       vi  \n",
       "...                                                   ...      ...  \n",
       "591070  Tất cả đều mới, đẹp và nên thơ. Phòng rộng, th...       vi  \n",
       "332938  Thái độ nhân viên tốt. Chu đáo hỗ trợ khách nh...       vi  \n",
       "392883  Ấn tượng đầu tiên là resort cảnh quan đẹp. Sân...       vi  \n",
       "113208  Tôi đến dùng bữa trưa ở khách sạn. khách sạn c...       vi  \n",
       "375793  Các villa hòa vào thiên nhiên của núi Chúa, tư...       vi  \n",
       "\n",
       "[38972 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.review = dataframe.review.tolist()\n",
    "        self.targets = dataframe.score.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # review = str(self.review[index])\n",
    "        # review = \" \".join(review.split())\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            self.review[index], \n",
    "            add_special_tokens=config.add_special_tokens, \n",
    "            return_attention_mask=config.return_attention_mask, \n",
    "            pad_to_max_length=config.pad_to_max_length,\n",
    "            max_length=config.seq_length, \n",
    "            # return_tensors=config.return_tensors\n",
    "        )\n",
    "        inputs[\"label\"] = self.targets[index]\n",
    "        inputs = {key: torch.tensor(value) for key, value in inputs.items()}\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomDataset(train_df, tokenizer)\n",
    "dataset_val = CustomDataset(val_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,   100,   100,   100,   100,  7001,  1010,   100, 16480,   100,\n",
       "           100,   100,   100,  1017,  2078,  2475,  2094,   100,   100,   100,\n",
       "           100,  1012,   100,   100,   100,   100,   100,   100,  1010,   100,\n",
       "         27699,  2078,  1060,  2319,  2232,  1010,   100,   100,  1012,   100,\n",
       "           100,   100,   100,   100,   100,   100,  1010,   100,   100,  1010,\n",
       "          2202,  2729,   100,  1047,  4048,   100,   100,   100,  4638,  2041,\n",
       "          1012,   100,   100,   100,   100,   100,   100, 17895,  2078,  1010,\n",
       "          1102,  2050,   100,   100,   100,   100,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(5)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\" Instructs how the DataLoader should process the data into a batch\"\"\"\n",
    "    \n",
    "#     text = [item['text'] for item in batch]\n",
    "#     labels = torch.stack([torch.tensor(item['label']) for item in batch])\n",
    "\n",
    "#     return {'text': text, 'tabular': tabular, 'label': labels}\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=config.batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(config.pretrained_model,\n",
    "                                                      num_labels=6,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False,\n",
    "                                                      cache_dir=config.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class BertTrainer:\n",
    "    \"\"\" A training and evaluation loop for PyTorch models with a BERT like architecture. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataloader,\n",
    "        eval_dataloader=None,\n",
    "        epochs=1,\n",
    "        lr=5e-04,\n",
    "        output_dir='./',\n",
    "        output_filename='model_state_dict.pt',\n",
    "        save=False,\n",
    "        tabular=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: torch.nn.Module: = A PyTorch model with a BERT like architecture,\n",
    "            tokenizer: = A BERT tokenizer for tokenizing text input,\n",
    "            train_dataloader: torch.utils.data.DataLoader = \n",
    "                A dataloader containing the training data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n",
    "            eval_dataloader: torch.utils.data.DataLoader = \n",
    "                A dataloader containing the evaluation data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n",
    "            epochs: int = An integer representing the number epochs to train,\n",
    "            lr: float = A float representing the learning rate for the optimizer,\n",
    "            output_dir: str = A string representing the directory path to save the model,\n",
    "            output_filename: string = A string representing the name of the file to save in the output directory,\n",
    "            save: bool = A boolean representing whether or not to save the model,\n",
    "            tabular: bool = A boolean representing whether or not the BERT model is modified to accept tabular data,\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.output_dir = output_dir\n",
    "        self.output_filename = output_filename\n",
    "        self.save = save\n",
    "        self.eval_loss = float('inf')  # tracks the lowest loss so as to only save the best model  \n",
    "        self.epochs = epochs\n",
    "        self.epoch_best_model = 0  # tracks which epoch the lowest loss is in so as to only save the best model\n",
    "        \n",
    "        \n",
    "    def train(self, evaluate=False):\n",
    "        \"\"\" Calls the batch iterator to train and optionally evaluate the model.\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            self.iteration(epoch, self.train_dataloader)\n",
    "            if evaluate and self.eval_dataloader is not None:\n",
    "                self.iteration(epoch, self.eval_dataloader, train=False)\n",
    "                \n",
    "    def evaluate(self):\n",
    "        \"\"\" Calls the batch iterator to evaluate the model.\"\"\"\n",
    "        epoch=0\n",
    "        self.iteration(epoch, self.eval_dataloader, train=False)\n",
    "    \n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\" Iterates through one epoch of training or evaluation\"\"\"\n",
    "        \n",
    "        # initialize variables\n",
    "        loss_accumulated = 0.\n",
    "        correct_accumulated = 0\n",
    "        samples_accumulated = 0\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        \n",
    "        self.model.train() if train else self.model.eval()\n",
    "        \n",
    "        # progress bar\n",
    "        mode = \"train\" if train else \"eval\"\n",
    "        batch_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=f\"EP ({mode}) {epoch}\",\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "        \n",
    "        # iterate through batches of the dataset\n",
    "        for i, batch in batch_iter:\n",
    "\n",
    "            batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n",
    "            batch_t[\"label\"] = batch[\"label\"].to(self.device)\n",
    "\n",
    "            logits = self.model(\n",
    "                input_ids=batch_t[\"input_ids\"], \n",
    "                token_type_ids=batch_t[\"token_type_ids\"], \n",
    "                attention_mask=batch_t[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            # calculate loss\n",
    "            loss = self.loss_fn(logits, batch_t[\"label\"])\n",
    "    \n",
    "            # compute gradient and and update weights\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # calculate the number of correct predictions\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct = preds.eq(batch_t[\"label\"]).sum().item()\n",
    "            \n",
    "            # accumulate batch metrics and outputs\n",
    "            loss_accumulated += loss.item()\n",
    "            correct_accumulated += correct\n",
    "            samples_accumulated += len(batch_t[\"label\"])\n",
    "            preds_all.append(preds.detach())\n",
    "            labels_all.append(batch_t['label'].detach())\n",
    "        \n",
    "        # concatenate all batch tensors into one tensor and move to cpu for compatibility with sklearn metrics\n",
    "        preds_all = torch.cat(preds_all, dim=0).cpu()\n",
    "        labels_all = torch.cat(labels_all, dim=0).cpu()\n",
    "        \n",
    "        # metrics\n",
    "        accuracy = accuracy_score(labels_all, preds_all)\n",
    "        precision = precision_score(labels_all, preds_all, average='macro')\n",
    "        recall = recall_score(labels_all, preds_all, average='macro')\n",
    "        f1 = f1_score(labels_all, preds_all, average='macro')\n",
    "        avg_loss_epoch = loss_accumulated / len(data_loader)\n",
    "        \n",
    "        # print metrics to console\n",
    "        print(\n",
    "            f\"samples={samples_accumulated}, \\\n",
    "            correct={correct_accumulated}, \\\n",
    "            acc={round(accuracy, 4)}, \\\n",
    "            recall={round(recall, 4)}, \\\n",
    "            prec={round(precision,4)}, \\\n",
    "            f1={round(f1, 4)}, \\\n",
    "            loss={round(avg_loss_epoch, 4)}\"\n",
    "        )    \n",
    "        \n",
    "        # save the model if the evaluation loss is lower than the previous best epoch \n",
    "        if self.save and not train and avg_loss_epoch < self.eval_loss:\n",
    "            \n",
    "            # create directory and filepaths\n",
    "            dir_path = Path(self.output_dir)\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = dir_path / f\"{self.output_filename}_epoch_{epoch}.pt\"\n",
    "            \n",
    "            # delete previous best model from hard drive\n",
    "            if epoch > 0:\n",
    "                file_path_best_model = dir_path / f\"{self.output_filename}_epoch_{self.epoch_best_model}.pt\"\n",
    "                !rm -f $file_path_best_model\n",
    "            \n",
    "            # save model\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            }, file_path)\n",
    "            \n",
    "            # update the new best loss and epoch\n",
    "            self.eval_loss = avg_loss_epoch\n",
    "            self.epoch_best_model = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels, label_dict):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = {key: value.to(config.device) for key, value in batch.items()}\n",
    "\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                       attention_mask=batch[\"attention_mask\"],\n",
    "                       token_type_ids=batch[\"token_type_ids\"],\n",
    "                       labels=batch[\"label\"])\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch['label'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "        \n",
    "    # calculate avareage val loss\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(dataloader_train) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ad05fd0faf4bd49d1c6a602d393969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1f41c37bfa45a3b5e041d0832df291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.5543593577245381\n",
      "Validation loss: 0.5118527162600967\n",
      "F1 Score (Weighted): 0.7909205673951524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef3d7908e1d40f295dedd538dfd281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.4774976786599175\n",
      "Validation loss: 0.498394967549864\n",
      "F1 Score (Weighted): 0.7956633935059885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf06dd9fcf94db0a37eb080c4464133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.4533945979743168\n",
      "Validation loss: 0.498394967549864\n",
      "F1 Score (Weighted): 0.7956633935059885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9427bd2e504daab56306926a6789cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.4542880081533407\n",
      "Validation loss: 0.498394967549864\n",
      "F1 Score (Weighted): 0.7956633935059885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30b8e43f5114c9a9de2b39cab117d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/phit/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.45326254604925664\n",
      "Validation loss: 0.498394967549864\n",
      "F1 Score (Weighted): 0.7956633935059885\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.to(config.device)\n",
    "    \n",
    "for epoch in tqdm(range(1, config.epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    # allows you to see the progress of the training \n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = {key: value.to(config.device) for key, value in batch.items()}\n",
    "        \n",
    "        logits = model(input_ids=batch[\"input_ids\"],\n",
    "                       attention_mask=batch[\"attention_mask\"],\n",
    "                       token_type_ids=batch[\"token_type_ids\"],\n",
    "                       labels=batch[\"label\"])\n",
    "        # print(logits[\"logits\"], batch[\"label\"])\n",
    "\n",
    "        loss = loss_fn(logits[\"logits\"], batch[\"label\"])\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    \n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}');\n",
    "# save model params and other configs \n",
    "with Path('params.json').open(\"w\") as f:\n",
    "    json.dump(params, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(config.pretrained_model,\n",
    "                                                      num_labels=6,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False,\n",
    "                                                      cache_dir=config.cache_dir)\n",
    "model.load_state_dict(torch.load('_BERT_epoch_5.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(config.device)\n",
    "data_iter = iter(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5 5 5 5 5 5 5 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 5 5 5 5 1 5 1 5]\n",
      " [5 5 4 3 5 5 4 5 2 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 1 5 1 5]]\n",
      "Class: A2\n",
      "Accuracy: 2/2\n",
      "\n",
      "Class: B1\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: B2\n",
      "Accuracy: 0/2\n",
      "\n",
      "Class: C1\n",
      "Accuracy: 0/2\n",
      "\n",
      "Class: C2\n",
      "Accuracy: 24/25\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)\n",
    "batch = {key: value.to(config.device) for key, value in batch.items()}\n",
    "with torch.no_grad():        \n",
    "    outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                token_type_ids=batch[\"token_type_ids\"],\n",
    "                labels=batch[\"label\"])\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "print(np.array([logits.argmax(axis=1), batch[\"label\"].detach().cpu().numpy()]))\n",
    "print(accuracy_per_class(logits, batch[\"label\"].detach().cpu().numpy(), {\"A1\": 0, \"A2\": 1, \"B1\": 2, \"B2\": 3, \"C1\": 4, \"C2\": 5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
